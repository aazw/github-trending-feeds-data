<?xml version='1.0' encoding='utf-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <id>https://aazw.github.io/github-trending-feeds/feeds/cuda/daily.atom</id>
  <title>GitHub Trending - cuda (daily)</title>
  <link href="https://aazw.github.io/github-trending-feeds/feeds/cuda/daily.atom" rel="self" />
  <link href="https://aazw.github.io/github-trending-feeds/" rel="alternate" />
  <icon>https://github.githubassets.com/favicons/favicon.svg</icon>
  <updated>2025-03-10T00:00:00</updated>
  <author>
    <name>aazw</name>
  </author>
  <entry>
    <id>https://github.com/HazyResearch/ThunderKittens#1741564800</id>
    <title>https://github.com/HazyResearch/ThunderKittens</title>
    <link href="https://github.com/HazyResearch/ThunderKittens" />
    <updated>2025-03-10T00:00:00</updated>
    <content type="text">Tile primitives for speedy kernels</content>
  </entry>
  <entry>
    <id>https://github.com/HigherOrderCO/HVM#1741564800</id>
    <title>https://github.com/HigherOrderCO/HVM</title>
    <link href="https://github.com/HigherOrderCO/HVM" />
    <updated>2025-03-10T00:00:00</updated>
    <content type="text">A massively parallel, optimal functional runtime in Rust</content>
  </entry>
  <entry>
    <id>https://github.com/Infatoshi/cuda-course#1741564800</id>
    <title>https://github.com/Infatoshi/cuda-course</title>
    <link href="https://github.com/Infatoshi/cuda-course" />
    <updated>2025-03-10T00:00:00</updated>
    <content type="text" />
  </entry>
  <entry>
    <id>https://github.com/NVIDIA/CUDALibrarySamples#1741564800</id>
    <title>https://github.com/NVIDIA/CUDALibrarySamples</title>
    <link href="https://github.com/NVIDIA/CUDALibrarySamples" />
    <updated>2025-03-10T00:00:00</updated>
    <content type="text">CUDA Library Samples</content>
  </entry>
  <entry>
    <id>https://github.com/NVIDIA/cub#1741564800</id>
    <title>https://github.com/NVIDIA/cub</title>
    <link href="https://github.com/NVIDIA/cub" />
    <updated>2025-03-10T00:00:00</updated>
    <content type="text">[ARCHIVED] Cooperative primitives for CUDA C++. See https://github.com/NVIDIA/cccl</content>
  </entry>
  <entry>
    <id>https://github.com/NVIDIA/nvbench#1741564800</id>
    <title>https://github.com/NVIDIA/nvbench</title>
    <link href="https://github.com/NVIDIA/nvbench" />
    <updated>2025-03-10T00:00:00</updated>
    <content type="text">CUDA Kernel Benchmarking Library</content>
  </entry>
  <entry>
    <id>https://github.com/ashawkey/diff-gaussian-rasterization#1741564800</id>
    <title>https://github.com/ashawkey/diff-gaussian-rasterization</title>
    <link href="https://github.com/ashawkey/diff-gaussian-rasterization" />
    <updated>2025-03-10T00:00:00</updated>
    <content type="text" />
  </entry>
  <entry>
    <id>https://github.com/drkennetz/cuda_examples#1741564800</id>
    <title>https://github.com/drkennetz/cuda_examples</title>
    <link href="https://github.com/drkennetz/cuda_examples" />
    <updated>2025-03-10T00:00:00</updated>
    <content type="text">Some CUDA example code with READMEs.</content>
  </entry>
  <entry>
    <id>https://github.com/flashinfer-ai/flashinfer#1741564800</id>
    <title>https://github.com/flashinfer-ai/flashinfer</title>
    <link href="https://github.com/flashinfer-ai/flashinfer" />
    <updated>2025-03-10T00:00:00</updated>
    <content type="text">FlashInfer: Kernel Library for LLM Serving</content>
  </entry>
  <entry>
    <id>https://github.com/karpathy/llm.c#1741564800</id>
    <title>https://github.com/karpathy/llm.c</title>
    <link href="https://github.com/karpathy/llm.c" />
    <updated>2025-03-10T00:00:00</updated>
    <content type="text">LLM training in simple, raw C/CUDA</content>
  </entry>
  <entry>
    <id>https://github.com/microsoft/TileFusion#1741564800</id>
    <title>https://github.com/microsoft/TileFusion</title>
    <link href="https://github.com/microsoft/TileFusion" />
    <updated>2025-03-10T00:00:00</updated>
    <content type="text">TileFusion is a highly efficient C++ macro kernel template library designed to elevate the level of abstraction in CUDA C for processing tiles.</content>
  </entry>
  <entry>
    <id>https://github.com/mit-han-lab/nunchaku#1741564800</id>
    <title>https://github.com/mit-han-lab/nunchaku</title>
    <link href="https://github.com/mit-han-lab/nunchaku" />
    <updated>2025-03-10T00:00:00</updated>
    <content type="text">[ICLR2025 Spotlight] SVDQuant: Absorbing Outliers by Low-Rank Components for 4-Bit Diffusion Models</content>
  </entry>
  <entry>
    <id>https://github.com/rapidsai/cugraph#1741564800</id>
    <title>https://github.com/rapidsai/cugraph</title>
    <link href="https://github.com/rapidsai/cugraph" />
    <updated>2025-03-10T00:00:00</updated>
    <content type="text">cuGraph - RAPIDS Graph Analytics Library</content>
  </entry>
  <entry>
    <id>https://github.com/rapidsai/cuvs#1741564800</id>
    <title>https://github.com/rapidsai/cuvs</title>
    <link href="https://github.com/rapidsai/cuvs" />
    <updated>2025-03-10T00:00:00</updated>
    <content type="text">cuVS - a library for vector search and clustering on the GPU</content>
  </entry>
  <entry>
    <id>https://github.com/rapidsai/raft#1741564800</id>
    <title>https://github.com/rapidsai/raft</title>
    <link href="https://github.com/rapidsai/raft" />
    <updated>2025-03-10T00:00:00</updated>
    <content type="text">RAFT contains fundamental widely-used algorithms and primitives for machine learning and information retrieval. The algorithms are CUDA-accelerated and form building blocks for more easily writing high performance applications.</content>
  </entry>
  <entry>
    <id>https://github.com/thu-ml/SageAttention#1741564800</id>
    <title>https://github.com/thu-ml/SageAttention</title>
    <link href="https://github.com/thu-ml/SageAttention" />
    <updated>2025-03-10T00:00:00</updated>
    <content type="text">Quantized Attention that achieves speedups of 2.1-3.1x and 2.7-5.1x compared to FlashAttention2 and xformers, respectively, without lossing end-to-end metrics across various models.</content>
  </entry>
</feed>