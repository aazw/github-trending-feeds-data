<?xml version='1.0' encoding='utf-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en">
  <id>https://aazw.github.io/github-trending-feeds/feeds/cuda/daily.atom</id>
  <title>GitHub Trending - cuda (daily)</title>
  <link href="https://aazw.github.io/github-trending-feeds/feeds/cuda/daily.atom" rel="self" />
  <link href="https://aazw.github.io/github-trending-feeds/" rel="alternate" />
  <icon>https://github.githubassets.com/favicons/favicon.svg</icon>
  <updated>2025-06-02T00:00:00</updated>
  <author>
    <name>aazw</name>
  </author>
  <entry>
    <id>https://github.com/Dao-AILab/causal-conv1d#1748822400</id>
    <title>https://github.com/Dao-AILab/causal-conv1d</title>
    <link href="https://github.com/Dao-AILab/causal-conv1d" />
    <updated>2025-06-02T00:00:00</updated>
    <content type="text">Causal depthwise conv1d in CUDA, with a PyTorch interface</content>
  </entry>
  <entry>
    <id>https://github.com/HazyResearch/ThunderKittens#1748822400</id>
    <title>https://github.com/HazyResearch/ThunderKittens</title>
    <link href="https://github.com/HazyResearch/ThunderKittens" />
    <updated>2025-06-02T00:00:00</updated>
    <content type="text">Tile primitives for speedy kernels</content>
  </entry>
  <entry>
    <id>https://github.com/Infatoshi/cuda-course#1748822400</id>
    <title>https://github.com/Infatoshi/cuda-course</title>
    <link href="https://github.com/Infatoshi/cuda-course" />
    <updated>2025-06-02T00:00:00</updated>
    <content type="text" />
  </entry>
  <entry>
    <id>https://github.com/NVIDIA/cub#1748822400</id>
    <title>https://github.com/NVIDIA/cub</title>
    <link href="https://github.com/NVIDIA/cub" />
    <updated>2025-06-02T00:00:00</updated>
    <content type="text">[ARCHIVED] Cooperative primitives for CUDA C++. See https://github.com/NVIDIA/cccl</content>
  </entry>
  <entry>
    <id>https://github.com/NVlabs/instant-ngp#1748822400</id>
    <title>https://github.com/NVlabs/instant-ngp</title>
    <link href="https://github.com/NVlabs/instant-ngp" />
    <updated>2025-06-02T00:00:00</updated>
    <content type="text">Instant neural graphics primitives: lightning fast NeRF and more</content>
  </entry>
  <entry>
    <id>https://github.com/ROCm/rccl-tests#1748822400</id>
    <title>https://github.com/ROCm/rccl-tests</title>
    <link href="https://github.com/ROCm/rccl-tests" />
    <updated>2025-06-02T00:00:00</updated>
    <content type="text">RCCL Performance Benchmark Tests</content>
  </entry>
  <entry>
    <id>https://github.com/SHI-Labs/NATTEN#1748822400</id>
    <title>https://github.com/SHI-Labs/NATTEN</title>
    <link href="https://github.com/SHI-Labs/NATTEN" />
    <updated>2025-06-02T00:00:00</updated>
    <content type="text">Neighborhood Attention Extension. Bringing attention to a neighborhood near you!</content>
  </entry>
  <entry>
    <id>https://github.com/accel-sim/gpu-app-collection#1748822400</id>
    <title>https://github.com/accel-sim/gpu-app-collection</title>
    <link href="https://github.com/accel-sim/gpu-app-collection" />
    <updated>2025-06-02T00:00:00</updated>
    <content type="text">A repository where GPU applications are aggregated using a common build flow that supports multiple CUDA versions.</content>
  </entry>
  <entry>
    <id>https://github.com/deepseek-ai/FlashMLA#1748822400</id>
    <title>https://github.com/deepseek-ai/FlashMLA</title>
    <link href="https://github.com/deepseek-ai/FlashMLA" />
    <updated>2025-06-02T00:00:00</updated>
    <content type="text">FlashMLA: Efficient MLA decoding kernels</content>
  </entry>
  <entry>
    <id>https://github.com/mit-han-lab/torchsparse#1748822400</id>
    <title>https://github.com/mit-han-lab/torchsparse</title>
    <link href="https://github.com/mit-han-lab/torchsparse" />
    <updated>2025-06-02T00:00:00</updated>
    <content type="text">[MICRO'23, MLSys'22] TorchSparse: Efficient Training and Inference Framework for Sparse Convolution on GPUs.</content>
  </entry>
  <entry>
    <id>https://github.com/nerfstudio-project/gsplat#1748822400</id>
    <title>https://github.com/nerfstudio-project/gsplat</title>
    <link href="https://github.com/nerfstudio-project/gsplat" />
    <updated>2025-06-02T00:00:00</updated>
    <content type="text">CUDA accelerated rasterization of gaussian splatting</content>
  </entry>
  <entry>
    <id>https://github.com/rapidsai/cugraph#1748822400</id>
    <title>https://github.com/rapidsai/cugraph</title>
    <link href="https://github.com/rapidsai/cugraph" />
    <updated>2025-06-02T00:00:00</updated>
    <content type="text">cuGraph - RAPIDS Graph Analytics Library</content>
  </entry>
  <entry>
    <id>https://github.com/rapidsai/cugraph-gnn#1748822400</id>
    <title>https://github.com/rapidsai/cugraph-gnn</title>
    <link href="https://github.com/rapidsai/cugraph-gnn" />
    <updated>2025-06-02T00:00:00</updated>
    <content type="text" />
  </entry>
  <entry>
    <id>https://github.com/rapidsai/cuvs#1748822400</id>
    <title>https://github.com/rapidsai/cuvs</title>
    <link href="https://github.com/rapidsai/cuvs" />
    <updated>2025-06-02T00:00:00</updated>
    <content type="text">cuVS - a library for vector search and clustering on the GPU</content>
  </entry>
  <entry>
    <id>https://github.com/rapidsai/raft#1748822400</id>
    <title>https://github.com/rapidsai/raft</title>
    <link href="https://github.com/rapidsai/raft" />
    <updated>2025-06-02T00:00:00</updated>
    <content type="text">RAFT contains fundamental widely-used algorithms and primitives for machine learning and information retrieval. The algorithms are CUDA-accelerated and form building blocks for more easily writing high performance applications.</content>
  </entry>
  <entry>
    <id>https://github.com/tgale96/grouped_gemm#1748822400</id>
    <title>https://github.com/tgale96/grouped_gemm</title>
    <link href="https://github.com/tgale96/grouped_gemm" />
    <updated>2025-06-02T00:00:00</updated>
    <content type="text">PyTorch bindings for CUTLASS grouped GEMM.</content>
  </entry>
  <entry>
    <id>https://github.com/thu-ml/SageAttention#1748822400</id>
    <title>https://github.com/thu-ml/SageAttention</title>
    <link href="https://github.com/thu-ml/SageAttention" />
    <updated>2025-06-02T00:00:00</updated>
    <content type="text">Quantized Attention achieves speedup of 2-5x and 3-11x compared to FlashAttention and xformers, without lossing end-to-end metrics across language, image, and video models.</content>
  </entry>
</feed>